device:
  accelerator: 'gpu'
  precision: '16-mixed'
  strategy: 'ddp'  # Distributed training 
memory:
  batch_size: 64
  prefetch_factor: 2
  pin_memory: true
optimization:
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  benchmark: true  # For fixed input sizes
workers:
  num_workers: 8
  persistent_workers: true